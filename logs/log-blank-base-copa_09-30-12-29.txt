using world size: 1 and model-parallel size: 1 
 > using dynamic loss scaling
> padded vocab (size: 30524) with 68 dummy tokens (new size: 30592)
> found end-of-document token: 0
Returning 500 test examples with label dist.: [(None, 500)]
flow.env.get_world_size() >>>>>  1 flow.env.get_rank() >>>>> 0
arguments:
  transformer_xl ............... False
  pretrained_bert .............. False
  encoder_decoder .............. False
  attention_dropout ............ 0.1
  num_attention_heads .......... 12
  hidden_size .................. 768
  intermediate_size ............ None
  num_layers ................... 12
  layernorm_epsilon ............ 1e-05
  hidden_dropout ............... 0.1
  output_dropout ............... 0.1
  max_position_embeddings ...... 512
  vocab_size ................... 30592
  deep_init .................... False
  make_vocab_size_divisible_by . 128
  cpu_optimizer ................ False
  cpu_torch_adam ............... False
  fp16 ......................... True
  fp32_embedding ............... False
  fp32_layernorm ............... False
  fp32_tokentypes .............. False
  fp32_allreduce ............... False
  hysteresis ................... 2
  loss_scale ................... None
  loss_scale_window ............ 1000
  min_scale .................... 1
  attention_scale .............. 1.0
  experiment_name .............. blank-base-copa_09-30-12-29
  batch_size ................... 4
  gradient_accumulation_steps .. 2
  weight_decay ................. 0.01
  checkpoint_activations ....... True
  checkpoint_num_layers ........ 1
  deepspeed_activation_checkpointing  False
  epochs ....................... -1
  clip_grad .................... 1.0
  train_iters .................. 0
  label_smoothing .............. 0.0
  log_interval ................. 20
  summary_dir .................. 
  seed ......................... 1234
  reset_position_ids ........... False
  reset_attention_mask ......... False
  lr_decay_iters ............... None
  lr_decay_style ............... linear
  lr_decay_ratio ............... 0.1
  lr ........................... 5e-06
  warmup ....................... 0.1
  switch_linear ................ False
  save ......................... /dataset/lichunyou/GLM/GLM_copa_oneflow/copa_model/blank-base-copa_09-30-12-29
  new_save_directory ........... False
  save_epoch ................... 100000
  save_interval ................ 10000
  no_save_optim ................ False
  no_save_rng .................. False
  load ......................... None
  no_load_optim ................ True
  no_load_rng .................. False
  no_load_lr_scheduler ......... True
  no_deepspeed_load ............ False
  finetune ..................... True
  resume_dataloader ............ False
  distributed_backend .......... nccl
  DDP_impl ..................... torch
  local_rank ................... None
  block_lm ..................... True
  masked_lm .................... False
  bert_prob .................... 0.5
  gpt_infill_prob .............. 0.5
  gpt_min_ratio ................ 0.5
  gap_sentence_prob ............ 0.0
  gap_sentence_ratio ........... 0.15
  avg_block_length ............. 3
  short_seq_prob ............... 0.0
  single_span_prob ............. 0.0
  task_mask .................... False
  no_shuffle_block ............. False
  no_block_position ............ False
  sentinel_token ............... False
  block_mask_prob .............. 0.0
  context_mask_ratio ........... 0.0
  random_position .............. False
  eval_batch_size .............. 4
  eval_iters ................... 100
  eval_interval ................ 1000
  eval_epoch ................... 1
  eval_seq_length .............. None
  eval_max_preds_per_seq ....... None
  overlapping_eval ............. 32
  temperature .................. 1.0
  top_p ........................ 0.0
  top_k ........................ 0
  out_seq_length ............... 256
  num_beams .................... 1
  length_penalty ............... 0.0
  no_repeat_ngram_size ......... 0
  min_tgt_length ............... 0
  select_topk .................. False
  blank_maskratio .............. 0.1
  model_parallel_size .......... 1
  shuffle ...................... False
  filter_english ............... False
  train_data ................... None
  valid_data ................... None
  test_data .................... None
  data_dir ..................... /dataset/lichunyou/GLM/GLM_copa_oneflow/dataset/COPA
  input_data_sizes_file ........ sizes.txt
  delim ........................ ,
  text_key ..................... sentence
  eval_text_key ................ None
  split ........................ 1000,1,1
  no_lazy_loader ............... False
  half_lazy_loader ............. False
  loader_scatter ............... None
  loose_json ................... False
  presplit_sentences ........... False
  num_workers .................. 1
  tokenizer_model_type ......... bert-base-uncased
  tokenizer_path ............... tokenizer.model
  tokenizer_type ............... BertWordPieceTokenizer
  no_pre_tokenize .............. False
  cache_dir .................... None
  use_tfrecords ................ False
  seq_length ................... 256
  mem_length ................... 0
  max_preds_per_seq ............ None
  non_sentence_start ........... 0.0
  sample_one_document .......... False
  load_splits .................. None
  save_splits .................. None
  save_test_data ............... None
  multi_task_data .............. None
  multi_task_ratio ............. 0.0
  multi_seq_length ............. None
  multi_batch_size ............. None
  task ......................... COPA
  load_pretrained .............. /dataset/lichunyou/GLM/GLM_copa_oneflow/copa_model/blank-base-copa_08-25-23-55
  pool_token ................... cls
  cloze_eval ................... True
  multi_token .................. False
  segment_length ............... 0
  loss_func .................... cross_entropy
  block_lm_ratio ............... 0.0
  adapet ....................... False
  pattern_id ................... 0
  fast_decode .................. False
  few_superglue ................ False
  eval_valid ................... False
  validation_metric ............ None
  unidirectional ............... False
  src_seq_length ............... None
  tgt_seq_length ............... None
  adam_beta1 ................... 0.9
  adam_beta2 ................... 0.999
  adam_eps ..................... 1e-08
  optimizer .................... adam
  wsc_negative ................. False
  overwrite .................... True
  no_validation ................ False
  continuous_prompt ............ False
  num_prompt_tokens ............ 0
  prompt_func .................. lstm
  freeze_transformer ........... False
  tune_prefix_layers ........... None
  prefix_prompt ................ 0
  prompt_init .................. False
  deepspeed .................... True
  deepspeed_config ............. config_tasks/config_blocklm_10B.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  cuda ......................... True
  rank ......................... 0
  world_size ................... 1
  dynamic_loss_scale ........... True
  eod_token .................... 0
  variable_num_choices ......... False
  iteration .................... 0
  log_dir ...................... runs/blank-base-copa_09-30-12-29
done with setups ...
time (ms) | train/valid/test dataset/dataloder: 0.29 | callback function: 7.64 | model and optimizer: 2899.91 | pretrained checkpoint: 1977.37
training ...
ssss
atten
Traceback (most recent call last):
  File "finetune_glm.py", line 484, in <module>
    main(args)
  File "/dataset/lichunyou/GLM/GLM_copa_oneflow/tasks/superglue/finetune.py", line 115, in main
    end_of_epoch_callback_provider=metrics_func_provider  #metrics_func_provider
  File "/dataset/lichunyou/GLM/GLM_copa_oneflow/finetune_glm.py", line 451, in finetune
    score_dict = end_of_train_callback(model, epoch=-1, output_predictions=True)
  File "/dataset/lichunyou/GLM/GLM_copa_oneflow/tasks/eval_utils.py", line 95, in metrics_func
    predictions, labels, examples = multichoice_evaluate(model, dataloader, example_dict, args)
  File "/dataset/lichunyou/GLM/GLM_copa_oneflow/tasks/eval_utils.py", line 193, in multichoice_evaluate
    logits, *mems = model(*inputs)
  File "/dataset/lichunyou/oneflow_src/oneflow/python/oneflow/nn/module.py", line 81, in __call__
    res = self.forward(*args, **kwargs)
  File "/dataset/lichunyou/GLM/GLM_copa_oneflow/model/downstream.py", line 41, in forward
    outputs, *mems = self.model(input_ids, position_ids, attention_mask, prompt_pos=prompt_pos)
  File "/dataset/lichunyou/oneflow_src/oneflow/python/oneflow/nn/module.py", line 81, in __call__
    res = self.forward(*args, **kwargs)
  File "/dataset/lichunyou/GLM/GLM_copa_oneflow/model/modeling_glm.py", line 110, in forward
    return_memory=return_memory, detach_memory=detach_memory)
  File "/dataset/lichunyou/oneflow_src/oneflow/python/oneflow/nn/module.py", line 81, in __call__
    res = self.forward(*args, **kwargs)
  File "/dataset/lichunyou/GLM/GLM_copa_oneflow/mpu/transformer.py", line 680, in forward
    hidden_states = custom(l, l + chunk_length)(*args)
  File "/dataset/lichunyou/GLM/GLM_copa_oneflow/mpu/transformer.py", line 656, in custom_forward
    x_ = layer(x_, *inputs, mem=mem_i_)
  File "/dataset/lichunyou/oneflow_src/oneflow/python/oneflow/nn/module.py", line 81, in __call__
    res = self.forward(*args, **kwargs)
  File "/dataset/lichunyou/GLM/GLM_copa_oneflow/mpu/transformer.py", line 428, in forward
    attention_output = self.attention(layernorm_output, ltor_mask, position_embeddings, r_w_bias, r_r_bias, mem)
  File "/dataset/lichunyou/oneflow_src/oneflow/python/oneflow/nn/module.py", line 81, in __call__
    res = self.forward(*args, **kwargs)
  File "/dataset/lichunyou/GLM/GLM_copa_oneflow/mpu/transformer.py", line 262, in forward
    with get_cuda_rng_tracker().fork():
  File "/dataset/lichunyou/conda_env/miniconda3/envs/p7/lib/python3.7/contextlib.py", line 112, in __enter__
    return next(self.gen)
  File "/dataset/lichunyou/GLM/GLM_copa_oneflow/mpu/random.py", line 174, in fork
    raise Exception('cuda rng state {} is not added'.format(name))
Exception: cuda rng state model-parallel-rng is not added
