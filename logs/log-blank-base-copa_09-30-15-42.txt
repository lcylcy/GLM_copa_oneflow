using world size: 1 and model-parallel size: 1 
 > using dynamic loss scaling
> padded vocab (size: 30524) with 68 dummy tokens (new size: 30592)
> found end-of-document token: 0
Returning 500 test examples with label dist.: [(None, 500)]
flow.env.get_world_size() >>>>>  1 flow.env.get_rank() >>>>> 0
arguments:
  transformer_xl ............... False
  pretrained_bert .............. False
  encoder_decoder .............. False
  attention_dropout ............ 0.1
  num_attention_heads .......... 12
  hidden_size .................. 768
  intermediate_size ............ None
  num_layers ................... 12
  layernorm_epsilon ............ 1e-05
  hidden_dropout ............... 0.1
  output_dropout ............... 0.1
  max_position_embeddings ...... 512
  vocab_size ................... 30592
  deep_init .................... False
  make_vocab_size_divisible_by . 128
  cpu_optimizer ................ False
  cpu_torch_adam ............... False
  fp16 ......................... True
  fp32_embedding ............... False
  fp32_layernorm ............... False
  fp32_tokentypes .............. False
  fp32_allreduce ............... False
  hysteresis ................... 2
  loss_scale ................... None
  loss_scale_window ............ 1000
  min_scale .................... 1
  attention_scale .............. 1.0
  experiment_name .............. blank-base-copa_09-30-15-42
  batch_size ................... 4
  gradient_accumulation_steps .. 2
  weight_decay ................. 0.01
  checkpoint_activations ....... True
  checkpoint_num_layers ........ 1
  deepspeed_activation_checkpointing  False
  epochs ....................... -1
  clip_grad .................... 1.0
  train_iters .................. 0
  label_smoothing .............. 0.0
  log_interval ................. 20
  summary_dir .................. 
  seed ......................... 1234
  reset_position_ids ........... False
  reset_attention_mask ......... False
  lr_decay_iters ............... None
  lr_decay_style ............... linear
  lr_decay_ratio ............... 0.1
  lr ........................... 5e-06
  warmup ....................... 0.1
  switch_linear ................ False
  save ......................... /dataset/lichunyou/GLM/GLM_copa_oneflow/copa_model/blank-base-copa_09-30-15-42
  new_save_directory ........... False
  save_epoch ................... 100000
  save_interval ................ 10000
  no_save_optim ................ False
  no_save_rng .................. False
  load ......................... None
  no_load_optim ................ True
  no_load_rng .................. False
  no_load_lr_scheduler ......... True
  no_deepspeed_load ............ False
  finetune ..................... True
  resume_dataloader ............ False
  distributed_backend .......... nccl
  DDP_impl ..................... torch
  local_rank ................... None
  block_lm ..................... True
  masked_lm .................... False
  bert_prob .................... 0.5
  gpt_infill_prob .............. 0.5
  gpt_min_ratio ................ 0.5
  gap_sentence_prob ............ 0.0
  gap_sentence_ratio ........... 0.15
  avg_block_length ............. 3
  short_seq_prob ............... 0.0
  single_span_prob ............. 0.0
  task_mask .................... False
  no_shuffle_block ............. False
  no_block_position ............ False
  sentinel_token ............... False
  block_mask_prob .............. 0.0
  context_mask_ratio ........... 0.0
  random_position .............. False
  eval_batch_size .............. 4
  eval_iters ................... 100
  eval_interval ................ 1000
  eval_epoch ................... 1
  eval_seq_length .............. None
  eval_max_preds_per_seq ....... None
  overlapping_eval ............. 32
  temperature .................. 1.0
  top_p ........................ 0.0
  top_k ........................ 0
  out_seq_length ............... 256
  num_beams .................... 1
  length_penalty ............... 0.0
  no_repeat_ngram_size ......... 0
  min_tgt_length ............... 0
  select_topk .................. False
  blank_maskratio .............. 0.1
  model_parallel_size .......... 1
  shuffle ...................... False
  filter_english ............... False
  train_data ................... None
  valid_data ................... None
  test_data .................... None
  data_dir ..................... /dataset/lichunyou/GLM/GLM_copa_oneflow/dataset/COPA
  input_data_sizes_file ........ sizes.txt
  delim ........................ ,
  text_key ..................... sentence
  eval_text_key ................ None
  split ........................ 1000,1,1
  no_lazy_loader ............... False
  half_lazy_loader ............. False
  loader_scatter ............... None
  loose_json ................... False
  presplit_sentences ........... False
  num_workers .................. 1
  tokenizer_model_type ......... bert-base-uncased
  tokenizer_path ............... tokenizer.model
  tokenizer_type ............... BertWordPieceTokenizer
  no_pre_tokenize .............. False
  cache_dir .................... None
  use_tfrecords ................ False
  seq_length ................... 256
  mem_length ................... 0
  max_preds_per_seq ............ None
  non_sentence_start ........... 0.0
  sample_one_document .......... False
  load_splits .................. None
  save_splits .................. None
  save_test_data ............... None
  multi_task_data .............. None
  multi_task_ratio ............. 0.0
  multi_seq_length ............. None
  multi_batch_size ............. None
  task ......................... COPA
  load_pretrained .............. /dataset/lichunyou/GLM/GLM_copa_oneflow/copa_model/blank-base-copa_08-25-23-55
  pool_token ................... cls
  cloze_eval ................... True
  multi_token .................. False
  segment_length ............... 0
  loss_func .................... cross_entropy
  block_lm_ratio ............... 0.0
  adapet ....................... False
  pattern_id ................... 0
  fast_decode .................. False
  few_superglue ................ False
  eval_valid ................... False
  validation_metric ............ None
  unidirectional ............... False
  src_seq_length ............... None
  tgt_seq_length ............... None
  adam_beta1 ................... 0.9
  adam_beta2 ................... 0.999
  adam_eps ..................... 1e-08
  optimizer .................... adam
  wsc_negative ................. False
  overwrite .................... True
  no_validation ................ False
  continuous_prompt ............ False
  num_prompt_tokens ............ 0
  prompt_func .................. lstm
  freeze_transformer ........... False
  tune_prefix_layers ........... None
  prefix_prompt ................ 0
  prompt_init .................. False
  deepspeed .................... True
  deepspeed_config ............. config_tasks/config_blocklm_10B.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  cuda ......................... True
  rank ......................... 0
  world_size ................... 1
  dynamic_loss_scale ........... True
  eod_token .................... 0
  variable_num_choices ......... False
  iteration .................... 0
  log_dir ...................... runs/blank-base-copa_09-30-15-42
done with setups ...
time (ms) | train/valid/test dataset/dataloder: 0.19 | callback function: 6.09 | model and optimizer: 2854.19 | pretrained checkpoint: 3931.74
training ...
tttt
(tensor([[[ 1.0983e+00,  1.1518e+00,  2.7661e-01,  ..., -1.6141e-02,
           1.5260e+00,  1.8653e+00],
         [ 1.1502e+00,  1.1097e+00,  2.0222e-01,  ..., -3.1285e-03,
           1.4470e+00,  1.9657e+00],
         [ 8.7173e-01,  8.0789e-01,  5.1147e-01,  ...,  3.3320e-03,
           1.4549e+00,  2.3349e+00],
         ...,
         [ 1.5650e+00,  1.0419e+00, -6.2946e-01,  ...,  7.5042e-01,
           1.2778e+00,  1.5162e+00],
         [ 6.5907e-01,  3.5798e+00, -1.4165e+00,  ..., -1.1796e+00,
           2.4756e+00,  6.0968e-01],
         [ 1.5650e+00,  1.0419e+00, -6.2946e-01,  ...,  7.5042e-01,
           1.2778e+00,  1.5162e+00]],

        [[ 1.0985e+00,  1.1520e+00,  2.7692e-01,  ..., -1.6520e-02,
           1.5262e+00,  1.8655e+00],
         [ 2.4074e+00,  6.2805e-01, -5.2464e-01,  ...,  7.4822e-01,
           2.1133e+00,  1.3348e+00],
         [ 1.1251e+00,  1.0100e+00,  1.2913e-01,  ...,  3.0795e-03,
           1.5138e+00,  1.9000e+00],
         ...,
         [-1.9911e+00,  1.2863e+00,  1.3076e-01,  ..., -1.8450e+00,
          -1.7695e-01, -1.9264e+00],
         [ 1.1121e+00,  1.1491e+00,  2.7224e-01,  ..., -2.8163e-02,
           1.5280e+00,  1.8670e+00],
         [ 7.4874e-01,  2.5384e-01, -5.6491e-01,  ...,  7.9045e-02,
           9.7410e-01,  2.0863e+00]],

        [[ 8.2483e-01,  1.0326e+00,  8.2632e-01,  ...,  2.4398e-02,
           1.4451e+00,  2.3051e+00],
         [ 6.1417e-01,  2.2238e-01, -4.6800e-01,  ..., -2.1275e-01,
           6.7883e-01,  2.3520e+00],
         [ 1.1424e+00,  1.0564e+00,  1.7707e-01,  ..., -3.2563e-02,
           1.4297e+00,  1.9752e+00],
         ...,
         [ 1.1121e+00,  1.1491e+00,  2.7224e-01,  ..., -2.8163e-02,
           1.5280e+00,  1.8670e+00],
         [ 1.1078e+00,  1.1472e+00,  2.7071e-01,  ..., -3.0712e-02,
           1.5267e+00,  1.8614e+00],
         [ 1.0892e+00,  7.2349e-01,  1.2635e+00,  ..., -1.4823e-01,
           9.8427e-01,  1.7493e+00]],

        ...,

        [[ 5.3588e-01,  5.0885e-01,  2.0192e-02,  ...,  3.8927e-02,
           5.4601e-01,  5.3336e-01],
         [ 1.0757e+00,  7.3219e-01,  1.1657e-01,  ..., -4.5952e-01,
           1.6900e+00,  1.0630e+00],
         [ 1.0411e+00,  1.0231e+00,  7.6261e-02,  ..., -7.2981e-02,
           1.4872e+00,  2.0866e+00],
         ...,
         [-8.1258e-01,  1.8798e+00,  1.9030e-01,  ..., -9.2471e-01,
           4.9173e-01, -3.3805e+00],
         [ 9.5144e-01,  1.0104e+00,  7.8796e-01,  ..., -6.4470e-02,
           1.3852e+00,  2.1965e+00],
         [ 3.1443e-01,  5.2757e-01,  5.0372e-02,  ...,  1.1886e-02,
           4.6951e-01,  5.2866e-01]],

        [[ 5.0655e-01,  4.9947e-01,  2.4592e-02,  ...,  3.6916e-02,
           5.3699e-01,  5.1791e-01],
         [ 6.3641e-01, -1.7992e-01, -7.3508e-01,  ...,  1.7058e+00,
           1.6250e+00,  2.3668e+00],
         [ 1.6415e+00,  1.3063e+00,  4.7175e-01,  ..., -5.0382e-01,
           1.4323e+00,  2.6501e+00],
         ...,
         [ 1.6976e+00,  1.4218e+00,  6.3425e-01,  ..., -4.6223e-01,
           1.4681e+00,  2.5802e+00],
         [ 1.1122e+00,  1.1492e+00,  2.7217e-01,  ..., -2.8203e-02,
           1.5280e+00,  1.8670e+00],
         [ 1.1121e+00,  1.1492e+00,  2.7218e-01,  ..., -2.8213e-02,
           1.5281e+00,  1.8671e+00]],

        [[ 1.1288e+00,  1.3126e+00,  2.6369e-01,  ..., -5.8999e-02,
           1.6586e+00,  2.0544e+00],
         [ 2.3500e+00,  1.3095e+00, -2.4819e-01,  ...,  1.0647e+00,
           2.1607e+00,  1.2223e+00],
         [ 1.2375e+00,  9.6673e-01,  1.0136e+00,  ..., -7.7423e-01,
           2.2054e+00,  1.4225e+00],
         ...,
         [-4.2092e-01,  2.3834e+00,  1.0257e+00,  ..., -5.4473e-01,
           9.8426e-01, -2.1355e+00],
         [ 1.1379e+00,  1.1496e+00,  2.6615e-01,  ..., -7.8617e-02,
           1.5182e+00,  1.8438e+00],
         [-3.1509e+00,  1.3314e+00,  5.6510e-01,  ...,  2.0868e+00,
          -2.7651e+00, -1.8062e+00]]], device='cuda:0', dtype=oneflow.float32), [])
