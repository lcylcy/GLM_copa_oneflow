using world size: 1 and model-parallel size: 1 
 > using dynamic loss scaling
> padded vocab (size: 30524) with 68 dummy tokens (new size: 30592)
> found end-of-document token: 0
Returning 500 test examples with label dist.: [(None, 500)]
flow.env.get_world_size() >>>>>  1 flow.env.get_rank() >>>>> 0
arguments:
  transformer_xl ............... False
  pretrained_bert .............. False
  encoder_decoder .............. False
  attention_dropout ............ 0.1
  num_attention_heads .......... 12
  hidden_size .................. 768
  intermediate_size ............ None
  num_layers ................... 12
  layernorm_epsilon ............ 1e-05
  hidden_dropout ............... 0.1
  output_dropout ............... 0.1
  max_position_embeddings ...... 512
  vocab_size ................... 30592
  deep_init .................... False
  make_vocab_size_divisible_by . 128
  cpu_optimizer ................ False
  cpu_torch_adam ............... False
  fp16 ......................... True
  fp32_embedding ............... False
  fp32_layernorm ............... False
  fp32_tokentypes .............. False
  fp32_allreduce ............... False
  hysteresis ................... 2
  loss_scale ................... None
  loss_scale_window ............ 1000
  min_scale .................... 1
  attention_scale .............. 1.0
  experiment_name .............. blank-base-copa_09-30-13-18
  batch_size ................... 4
  gradient_accumulation_steps .. 2
  weight_decay ................. 0.01
  checkpoint_activations ....... True
  checkpoint_num_layers ........ 1
  deepspeed_activation_checkpointing  False
  epochs ....................... -1
  clip_grad .................... 1.0
  train_iters .................. 0
  label_smoothing .............. 0.0
  log_interval ................. 20
  summary_dir .................. 
  seed ......................... 1234
  reset_position_ids ........... False
  reset_attention_mask ......... False
  lr_decay_iters ............... None
  lr_decay_style ............... linear
  lr_decay_ratio ............... 0.1
  lr ........................... 5e-06
  warmup ....................... 0.1
  switch_linear ................ False
  save ......................... /dataset/lichunyou/GLM/GLM_copa_oneflow/copa_model/blank-base-copa_09-30-13-18
  new_save_directory ........... False
  save_epoch ................... 100000
  save_interval ................ 10000
  no_save_optim ................ False
  no_save_rng .................. False
  load ......................... None
  no_load_optim ................ True
  no_load_rng .................. False
  no_load_lr_scheduler ......... True
  no_deepspeed_load ............ False
  finetune ..................... True
  resume_dataloader ............ False
  distributed_backend .......... nccl
  DDP_impl ..................... torch
  local_rank ................... None
  block_lm ..................... True
  masked_lm .................... False
  bert_prob .................... 0.5
  gpt_infill_prob .............. 0.5
  gpt_min_ratio ................ 0.5
  gap_sentence_prob ............ 0.0
  gap_sentence_ratio ........... 0.15
  avg_block_length ............. 3
  short_seq_prob ............... 0.0
  single_span_prob ............. 0.0
  task_mask .................... False
  no_shuffle_block ............. False
  no_block_position ............ False
  sentinel_token ............... False
  block_mask_prob .............. 0.0
  context_mask_ratio ........... 0.0
  random_position .............. False
  eval_batch_size .............. 4
  eval_iters ................... 100
  eval_interval ................ 1000
  eval_epoch ................... 1
  eval_seq_length .............. None
  eval_max_preds_per_seq ....... None
  overlapping_eval ............. 32
  temperature .................. 1.0
  top_p ........................ 0.0
  top_k ........................ 0
  out_seq_length ............... 256
  num_beams .................... 1
  length_penalty ............... 0.0
  no_repeat_ngram_size ......... 0
  min_tgt_length ............... 0
  select_topk .................. False
  blank_maskratio .............. 0.1
  model_parallel_size .......... 1
  shuffle ...................... False
  filter_english ............... False
  train_data ................... None
  valid_data ................... None
  test_data .................... None
  data_dir ..................... /dataset/lichunyou/GLM/GLM_copa_oneflow/dataset/COPA
  input_data_sizes_file ........ sizes.txt
  delim ........................ ,
  text_key ..................... sentence
  eval_text_key ................ None
  split ........................ 1000,1,1
  no_lazy_loader ............... False
  half_lazy_loader ............. False
  loader_scatter ............... None
  loose_json ................... False
  presplit_sentences ........... False
  num_workers .................. 1
  tokenizer_model_type ......... bert-base-uncased
  tokenizer_path ............... tokenizer.model
  tokenizer_type ............... BertWordPieceTokenizer
  no_pre_tokenize .............. False
  cache_dir .................... None
  use_tfrecords ................ False
  seq_length ................... 256
  mem_length ................... 0
  max_preds_per_seq ............ None
  non_sentence_start ........... 0.0
  sample_one_document .......... False
  load_splits .................. None
  save_splits .................. None
  save_test_data ............... None
  multi_task_data .............. None
  multi_task_ratio ............. 0.0
  multi_seq_length ............. None
  multi_batch_size ............. None
  task ......................... COPA
  load_pretrained .............. /dataset/lichunyou/GLM/GLM_copa_oneflow/copa_model/blank-base-copa_08-25-23-55
  pool_token ................... cls
  cloze_eval ................... True
  multi_token .................. False
  segment_length ............... 0
  loss_func .................... cross_entropy
  block_lm_ratio ............... 0.0
  adapet ....................... False
  pattern_id ................... 0
  fast_decode .................. False
  few_superglue ................ False
  eval_valid ................... False
  validation_metric ............ None
  unidirectional ............... False
  src_seq_length ............... None
  tgt_seq_length ............... None
  adam_beta1 ................... 0.9
  adam_beta2 ................... 0.999
  adam_eps ..................... 1e-08
  optimizer .................... adam
  wsc_negative ................. False
  overwrite .................... True
  no_validation ................ False
  continuous_prompt ............ False
  num_prompt_tokens ............ 0
  prompt_func .................. lstm
  freeze_transformer ........... False
  tune_prefix_layers ........... None
  prefix_prompt ................ 0
  prompt_init .................. False
  deepspeed .................... True
  deepspeed_config ............. config_tasks/config_blocklm_10B.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  cuda ......................... True
  rank ......................... 0
  world_size ................... 1
  dynamic_loss_scale ........... True
  eod_token .................... 0
  variable_num_choices ......... False
  iteration .................... 0
  log_dir ...................... runs/blank-base-copa_09-30-13-18
done with setups ...
time (ms) | train/valid/test dataset/dataloder: 0.30 | callback function: 12.62 | model and optimizer: 2849.51 | pretrained checkpoint: 4092.01
training ...
test-0
test-1
test-2
test-3
test-4
test-5
test-6
test-7
test-8
test-9
test-10
test-11
test-12
test-13
test-14
test-15
test-16
test-17
test-18
test-19
test-20
test-21
test-22
test-23
test-24
test-25
test-26
test-27
test-28
test-29
test-30
test-31
test-32
test-33
test-34
test-35
test-36
test-37
test-38
test-39
test-40
test-41
test-42
test-43
test-44
test-45
test-46
test-47
test-48
test-49
test-50
test-51
test-52
test-53
test-54
test-55
test-56
test-57
test-58
test-59
test-60
test-61
test-62
test-63
test-64
test-65
test-66
test-67
test-68
test-69
test-70
test-71
test-72
test-73
test-74
test-75
test-76
test-77
test-78
test-79
test-80
test-81
test-82
test-83
test-84
test-85
test-86
test-87
test-88
test-89
test-90
test-91
test-92
test-93
test-94
test-95
test-96
test-97
test-98
test-99
test-100
test-101
test-102
test-103
test-104
test-105
test-106
test-107
test-108
test-109
test-110
test-111
test-112
test-113
test-114
test-115
test-116
test-117
test-118
test-119
test-120
test-121
test-122
test-123
test-124
test-125
test-126
test-127
test-128
test-129
test-130
test-131
test-132
test-133
test-134
test-135
test-136
test-137
test-138
test-139
test-140
test-141
test-142
test-143
test-144
test-145
test-146
test-147
test-148
test-149
test-150
test-151
test-152
test-153
test-154
test-155
test-156
test-157
test-158
test-159
test-160
test-161
test-162
test-163
test-164
test-165
test-166
test-167
test-168
test-169
test-170
test-171
test-172
test-173
test-174
test-175
test-176
test-177
test-178
test-179
test-180
test-181
test-182
test-183
test-184
test-185
test-186
test-187
test-188
test-189
test-190
test-191
test-192
test-193
test-194
test-195
test-196
test-197
test-198
test-199
test-200
test-201
test-202
test-203
test-204
test-205
test-206
test-207
test-208
test-209
test-210
test-211
test-212
test-213
test-214
test-215
test-216
test-217
test-218
test-219
test-220
test-221
test-222
test-223
test-224
test-225
test-226
test-227
test-228
test-229
test-230
test-231
test-232
test-233
test-234
test-235
test-236
test-237
test-238
test-239
test-240
test-241
test-242
test-243
test-244
test-245
test-246
test-247
test-248
test-249
test-250
test-251
test-252
test-253
test-254
test-255
test-256
test-257
test-258
test-259
test-260
test-261
test-262
test-263
test-264
test-265
test-266
test-267
test-268
test-269
test-270
test-271
test-272
test-273
test-274
test-275
test-276
test-277
test-278
test-279
test-280
test-281
test-282
test-283
test-284
test-285
test-286
test-287
test-288
test-289
test-290
test-291
test-292
test-293
test-294
test-295
test-296
test-297
test-298
test-299
test-300
test-301
test-302
test-303
test-304
test-305
test-306
test-307
test-308
test-309
test-310
test-311
test-312
test-313
test-314
test-315
test-316
test-317
test-318
test-319
test-320
test-321
test-322
test-323
test-324
test-325
test-326
test-327
test-328
test-329
test-330
test-331
test-332
test-333
test-334
test-335
test-336
test-337
test-338
test-339
test-340
test-341
test-342
test-343
test-344
test-345
test-346
test-347
test-348
test-349
test-350
test-351
test-352
test-353
test-354
test-355
test-356
test-357
test-358
test-359
test-360
test-361
test-362
test-363
test-364
test-365
test-366
test-367
test-368
test-369
test-370
test-371
test-372
test-373
test-374
test-375
test-376
test-377
test-378
test-379
test-380
test-381
test-382
test-383
test-384
test-385
test-386
test-387
test-388
test-389
test-390
test-391
test-392
test-393
test-394
test-395
test-396
test-397
test-398
test-399
test-400
test-401
test-402
test-403
test-404
test-405
test-406
test-407
test-408
test-409
test-410
test-411
test-412
test-413
test-414
test-415
test-416
test-417
test-418
test-419
test-420
test-421
test-422
test-423
test-424
test-425
test-426
test-427
test-428
test-429
test-430
test-431
test-432
test-433
test-434
test-435
test-436
test-437
test-438
test-439
test-440
test-441
test-442
test-443
test-444
test-445
test-446
test-447
test-448
test-449
test-450
test-451
test-452
test-453
test-454
test-455
test-456
test-457
test-458
test-459
test-460
test-461
test-462
test-463
test-464
test-465
test-466
test-467
test-468
test-469
test-470
test-471
test-472
test-473
test-474
test-475
test-476
test-477
test-478
test-479
test-480
test-481
test-482
test-483
test-484
test-485
test-486
test-487
test-488
test-489
test-490
test-491
test-492
test-493
test-494
test-495
test-496
test-497
test-498
test-499
